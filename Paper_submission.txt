Title: LOOM: Personalized Learning Informed by Daily LLM Conversations Toward Long-Term Mastery via a Dynamic Learner Memory Graph

Abstract:
Abstract: Foundation models are increasingly used to personalize learning, yet many systems still assume fixed curricula or coarse progress signals, limiting alignment with learners’ day-to-day needs. At the other extreme, lightweight incidental systems offer flexible, in-the-moment content but rarely guide learners toward mastery. Prior work tends to privilege either continuity (maintaining a plan across sessions) or initiative (reacting to the moment), not both, leaving learners to navigate the trade-off between recency and trajectory—immediate relevance versus cumulative, goal-aligned progress. We present Loom, an agentic pipeline that infers evolving learner needs from recent LLM conversations and a dynamic learner graph, then assembles coherent learning materials personalized to the learner’s current needs, priorities, and understanding. These materials link adjacent concepts and surface gaps as tightly scoped modules that cumulatively advance broader goals, providing guidance and sustained progress while remaining responsive to new interests. We outline an end-to-end architecture that infers goals and gaps from conversational history, plans and sequences adaptive learning materials, and updates the dynamic learner memory graph; we also present a working prototype with cross-domain scenarios. We conclude with an evaluation plan focused on usability, alignment with perceived needs, and learning efficacy, and highlight how everyday interactions with LLMs can be transformed into scalable, guided learning.

Introduction:

General‑purpose chat assistants like ChatGPT are good at answering explicit questions, but they are not designed to guide learning. While general-purpose chat assistants typically accumulate rich context from frequent user interactions, they rarely leverage this proactively to structure personalized learning or help learners recognize their own “unknown unknowns”—knowledge gaps they sense but cannot explicitly name. This leaves learners bearing the burden of identifying next steps and integrating isolated insights into a coherent, sustained learning trajectory. The result is assistance that feels useful in the moment yet fragmented and disconnected over time.
Design tensions. LLM‑based learning tools already deliver on the pedagogy pillar—they explain, quiz, and give feedback—but in practice they tend to emphasize either continuity or initiative/flexibility, rarely both together. Systems that prioritize continuity maintain a plan or learner model across sessions, yet usually assume declared goals and update mainly inside the tutoring context; they seldom take the initiative to read a learner’s ongoing activity (e.g., everyday queries/chats) and propose adaptive, goal-aligned next steps when objectives are hazy under “unknown unknowns.” Conversely, tools that emphasize initiative/flexibility are responsive to the moment and good at helping learners start, but they rarely carry forward those micro‑steps into a durable trajectory that tracks mastery over days or weeks. This fragmentation places the cognitive burden on learners, forcing them to manually integrate immediate interactions into a meaningful, long-term trajectory—underscoring the need for a unified approach that seamlessly merges continuity, initiative, and pedagogy.
Prior LLM-based learning tools fall into three threads that each miss something essential: fall into three threads that each miss something essential: (1) chat‑based and target‑focused tutors that often track learning path over time, yet don’t watch users’ everyday activity; they adapt to the curriculum, not to evolving information needs or daily time/attention; (2) incidental learning systems that surface in‑situ content but do not scaffold these experiences toward durable, long-term understanding; and (3) memory‑augmented assistants that keep rich personal context yet are not pedagogical—they don’t track mastery or assemble lessons into a coherent progression. 
We propose LOOM, a personalized learning agent backed by a dynamic learner memory graph that observes everyday LLM conversations, infers evolving needs, and assembles coherent learning modules scoped to current priorities and understanding. LOOM links these modules in a dynamic learner graph to track mastery and weave new material into existing knowledge over time, unifying continuity and flexibility in a single pedagogical pipeline.
Our research questions are: 
RQ1: Can personalized learning driven by everyday LLM conversations better align with learners’ needs and boost engagement compared to a reactive chat baseline?
RQ2: Can LOOM achieve comparable learning outcomes while remaining responsive to evolving goals and interests?
RQ3: How does a dynamic learner memory graph influence learners’ sense of coherence and progress over time?
This paper makes three contributions:
System design and prototype. We introduce LOOM, a personalized learning agent underpinned by a dynamic learner memory graph; we describe its end-to-end pipeline, design rationale, and a working prototype demonstrating cross-domain scenarios.
Early user feedback. We present insights from a preliminary user evaluation—gathered via scenario walkthroughs and experience questionnaires—showing improved alignment with perceived needs, clarity of progression, and engagement.
Design implications and future directions. We distill lessons for integrating dynamic learner memory graphs into LLM-based learning tools, and outline next steps for scaling evaluations and extending to diverse learning domains.

Related Work
The development of personalized learning technologies leveraging large language models (LLMs) spans several prominent research streams, each addressing specific facets of personalization and learning but frequently neglecting critical dimensions such as sustained progression, adaptive responsiveness, and proactive guidance derived from everyday interactions.
General-Purpose Chat Assistants:
General-purpose conversational agents, notably ChatGPT, provide robust and contextually relevant answers to immediate user inquiries across diverse domains. These systems excel at responding to explicit questions in isolated interactions, yet rarely capitalize on extensive conversational histories to proactively support long-term mastery. Their capabilities typically remain limited to single-turn responsiveness, and they lack systematic strategies to guide incremental knowledge building or identify and address implicit learning gaps across multiple interactions (Chen et al., 2024; Liao & Yang, 2023).
Structured, Target-Focused Learning:
Structured learning platforms such as GenMentor, TutorLLM, and ChatTutor utilize advanced personalization strategies, including knowledge tracing, learner modeling, and retrieval-augmented generation (RAG), to systematically guide learners towards explicit, predefined learning goals. ChatTutor, for instance, employs a modular architecture composed of interaction, reflection, and reaction processes, systematically adapting course plans and quizzes based on learners' progress. Despite these capabilities, such structured approaches often lack the flexibility to spontaneously accommodate emerging interests or fluctuating learner engagement and attention levels. Consequently, these systems struggle to dynamically adapt to everyday learning contexts outside their predefined curricula, limiting their responsiveness to fluid learner priorities (Chen et al., 2024; Wang et al., 2024; Ji et al., 2024).
Incidental Learning Systems:
Platforms such as AiGet, G-VOILA, and VocabEncounter specialize in incidental microlearning, leveraging environmental cues, gaze patterns, and daily activities to surface timely, contextually relevant knowledge fragments. AiGet, for example, integrates seamlessly into everyday tasks by observing surroundings and user interactions to trigger incidental learning moments. Although these systems effectively promote immediate learner engagement and curiosity-driven exploration, they typically lack robust scaffolding mechanisms necessary for transforming incidental insights into structured, cumulative knowledge. Thus, their capability for supporting sustained, goal-oriented mastery remains inherently constrained (Wang et al., 2024; Draxler et al., 2022; Chen et al., 2015).
Memory-Augmented Personal Assistants:
Memory-enhanced assistants, exemplified by GUM and OmniQuery, maintain extensive user contexts and leverage rich historical interaction data for proactive, personalized information retrieval and augmentation. OmniQuery, in particular, supports context-aware multimodal memory retrieval tailored to personalized queries. However, these platforms generally lack explicit pedagogical structures for systematically organizing knowledge, evaluating mastery, and scaffolding learning experiences into coherent, progressively structured lessons. Their primary strengths lie in utility-oriented productivity enhancement rather than guiding structured educational trajectories or explicit learning objectives (Chen et al., 2024; Liao & Yang, 2023).
LOOM synthesizes the strengths from these diverse research streams by proactively utilizing everyday conversational interactions to infer evolving learner needs dynamically. It curates personalized, coherent learning modules attuned to immediate learner priorities and detected knowledge gaps. By leveraging a dynamic learner memory graph, LOOM effectively integrates adaptive responsiveness with structured pedagogical scaffolding, bridging existing gaps in personalized learning systems to promote coherent and sustained mastery.
